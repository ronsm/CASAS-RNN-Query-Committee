# HAR Query Committee

This repository can use models generated by other packages to form a committee of three learners. For the sake of demonstration, it support RNN models generated by the [CASAS Model Generator / AL Tools](https://github.com/ronsm/CASAS-Model-Generator-AL-Validator).

It does the following tasks: (i) generates predictions from those models in a real-time simulation of one prediction per second; (ii) evaluates disagreement between those models using Kullback-Leibler (KL) divergence using the last _T_ seconds (user defined parameter) of data; and (iii) engages in dialogue with the user to label data selected for querying.

This software forms part of a system to enable simulation of a real smart home with multiple HAR classifiers, to enable experiments in active learning.

## Install / Requirements

To install, run these commands:

```
git clone https://github.com/ronsm/HAR-Query-Committee.git
cd /directory/where/you/cloned/to
pip3 install -r requirements.txt
```

Note that if you require on an older version of TensorFlow (pre 2.0), this will upgrade your installation to the latest version if not using virtual environments.

Inside the ```query_select.py``` file, you must also configure the following parameters if you wish to change the window length or add models:
```
ROLLING_WINDOW = 30     # length of the rolling window for evaluation in seconds
NUM_LEARNERS = 3        # number of learners in the committee
```

### Configuration

Please ensure you set the configuration variables in the configuration area of ```query_process_control.py```. For example:

```
# set to True for real-time mode (1 second/sample)
self.real_time = False

# set to True for automatic labelling
self.oracle = True

# set to True for automated re-training
self.auto_al = True

# set a limit on the number of predictions (for debug)
self.max_predictions = 0

# enable/disable debug mode (more verbose)
self.debug = False

# select the dataset
self.dataset = "CASAS"
```

### Label Links

For any dataset, you must provide a ```label_links.txt``` file, as shown below:

```
├── data
│   ├── [dataset]
│   │   ├── label_links.txt
│   │   └── ...
│   └── ...
└── ...
```

This file relates a dataset to the pre-trained Activities of Daily Living (ADLs) natural language model. The pre-trained model is able to identify a range of common ADLs from textual input. For each activity label in the pre-trained model, you must provide the desination label in a given dataset, plus a semantic description of that label (used in dialogues).

An example, for the CASAS 'Milan' dataset, is given below. Note the format is ```ADL_label:CASAS_label:CASAS_description```:

```
other:other:doing something else
relaxing:relax:relaxing
working:work:working
studying:work:working
sleeping:sleep:sleeping
leaving_the_house:leave_home:going out
bathing:bathing:bathing
cooking:cook:cooking
eating:eat:eating
snacking:eat:eating
watching_TV:relax:relaxing
using_computer:other:doing something else
using_smartphone:other:doing something else
using_internet:other:doing something else
washing_dishes:work:working
reading:relax:relaxing
doing_laundry:work:working
shaving:bathing:bathing
brushing_teeth:bathing:bathing
talking_on_phone:relax:relaxing
listening_to_music:relax:relaxing
cleaning:work:working
conversing:relax:relaxing
hosting_guests:relax:relaxing
getting_dressed:other:doing something else
tidying_up:work:working
exercising:other:doing something else
hobby:other:doing something else
taking_medication:take_medicine:taking your medication
toileting:bed_to_toilet:using the toilet
```

### CASAS

You can either manually supply the test data, models, and labels generated by the [CASAS Model Generator / AL Tools](https://github.com/ronsm/CASAS-RNN-Model-Generator) to the program, or use automated re-training mode.

To enable auto mode, set ```self.auto_al``` to ```True``` in the ```query_process_control.py``` configuration area.

For manual mode, follow these instructions. For the test files, you must (rename and) place under the ```data/CASAS``` folder. You must also place the labels file for your chosen dataset in the same folder, e.g. for ```kyoto11``` you need ```kyoto11-labels.npy``` from the ```npy``` folder. Take this file and rename it to ```labels.py``` and place it in the ```data/CASAS``` folder. The folder should ultimately appear as follows:

```
├── data
│   ├── CASAS
│   │   ├── labels.npy
│   │   ├── label_links.txt
│   │   ├── x_test.csv
│   │   └── y_test.csv
│   └── ...
└── ...
```

For models, you must (rename and) place under the ```models/CASAS``` folder the ```.h5``` files as follow:
```
├── models
│   ├── CASAS
│   │   ├── biLSTM.h5
│   │   ├── CascadeLSTM.h5
│   │   └── LSTM.h5
│   └── ...
└── ...
```

## Usage

The ```query_process_control.py``` file orchestrates timing of operations. It determines when the next sample is loaded into the sample buffer and predictions made on that sample, before triggering an evaluation of the prediction buffer by the query evaluator. It uses Python's ```time.perf_counter``` to measure the time of these tasks to adjust sleep times and maintain the fixed time between samples.

### Committee Predict

Class files formatted as ```[DATASET]_committee_predict.py``` simply loads the provided models and makes predictions on sequential samples from the test set. It returns the probability predictions from each learner on the same sample of data.

### Query Select

File ```query_select.py``` has these main tasks:
* Maintain a sample buffer
* Maintains a rolling window from the head of the buffer (size determiend by ```ROLLING_WINDOW```)
* Evaluate the max disagreement (Kullback-Leibler divergence) between the learners over the fixed window

Max disagreement is found by calculating the consensus probabilities, measuring entropy of the consensus, and then by calculating the Kullback-Leibler divergence of each learner to the consensus prediction. Max disagreement is then the argmax of the learner KL divergence for each sample.

The program will generate a CSV log file in the ```logs``` folder, timestamped with the date/time that the program was run. The log file shows the class prediction from each learner, the ground truth value, the calculated max disagreement, and whether a query would be triggered.

### Dialogue Manager

The dialogue manager class in ```dialogue_manager.py``` is actived when a data point is selected for querying. It takes in the labels from the selected sample, and starts a dialogue with the user via the terminal.

It uses AIML and Spacy for NLU, and generates text using the responder class.
